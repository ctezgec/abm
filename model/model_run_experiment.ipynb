{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Run**\n",
    "\n",
    "In this file, the model run as designed experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and model\n",
    "from model import AdaptationModel\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up model parameters\n",
    "# run time: 100 years --> 400 quarters, so 400 ticks in total\n",
    "run_length = 400\n",
    "# set the number of replications \n",
    "replication_number = 5\n",
    "# set the number of households\n",
    "number_of_households = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sensitivity analysis\n",
    "subsidy_rate = [0.33]   # subsidy rate offered\n",
    "income_threshold = [3000, 6000] # agents which have lower income then the threshold eligible for subsidy\n",
    "saving_threshold = [0, 0.25, 0.75, 1] # agent calculate_saving(self) threshold, for saving or consuming\n",
    "flood_probabilities = [0.02, 0.07, 0.3] # different flood probabilities for harvey map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "model_experiment = defaultdict(list)\n",
    "\n",
    "monitor = 0\n",
    "for rate in saving_threshold:\n",
    "    for income_value in income_threshold:\n",
    "        for subsidy_value in subsidy_rate:\n",
    "            for flood_value in flood_probabilities:\n",
    "                for i in range(replication_number):\n",
    "                    seed = i  # set the seed\n",
    "                    monitor += 1\n",
    "                    print('this is run of', monitor)\n",
    "                    model = AdaptationModel(number_of_households=number_of_households, subsidy_rate=subsidy_value, income_threshold=income_value, saving_threshold=rate, harvey_probability=flood_value, flood_map_choice=\"harvey\", network=\"no_network\", seed=seed)\n",
    "                    for tick in range(run_length):\n",
    "                        model.step()\n",
    "                    model_data2 = model.datacollector.get_model_vars_dataframe()\n",
    "                    # Include all four variables in the key\n",
    "                    model_experiment[(i, rate, income_value, subsidy_value, flood_value)].append(model_data2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dictionary to hold the concatenated dataframes\n",
    "concatenated_dfs = {}\n",
    "\n",
    "# Concatenate the dataframes for each key\n",
    "for key, result_list in model_experiment.items():\n",
    "    # The key now includes all four variables\n",
    "    concatenated_dfs[key] = pd.concat(result_list)\n",
    "\n",
    "# Convert the dictionary of dataframes to a single dataframe\n",
    "# The key names will become column names\n",
    "model_dataframe_experiment = pd.concat(concatenated_dfs, names=['replication/seed', 'rate', 'income_value', 'subsidy_value', 'flood_value', 'Step'])\n",
    "\n",
    "# Reset the index\n",
    "model_dataframe_experiment.reset_index(inplace=True)\n",
    "\n",
    "# Save the results to csv files\n",
    "model_dataframe_experiment.to_csv(f\"../result_experiment/{'model_experiment'}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**run 2 and save**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters for the sensitivity analysis\n",
    "subsidy_rate = [0.33]   # subsidy rate offered\n",
    "income_threshold = [3000, 6000, 9000, 12000] # agents which have lower income then the threshold eligible for subsidy\n",
    "saving_threshold = [0, 0.25, 0.75, 1] # agent calculate_saving(self) threshold, for saving or consuming\n",
    "flood_probabilities = [0.02, 0.07, 0.3] # different flood probabilities for harvey map\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "model_experiment = defaultdict(list)\n",
    "\n",
    "monitor = 0\n",
    "for rate in saving_threshold:\n",
    "    for income_value in income_threshold:\n",
    "        for subsidy_value in subsidy_rate:\n",
    "            for flood_value in flood_probabilities:\n",
    "                for i in range(replication_number):\n",
    "                    seed = i  # set the seed\n",
    "                    monitor += 1\n",
    "                    print('this is run of', monitor)\n",
    "                    model = AdaptationModel(number_of_households=number_of_households, subsidy_rate=subsidy_value, income_threshold=income_value, saving_threshold=rate, harvey_probability=flood_value, flood_map_choice=\"harvey\", network=\"no_network\", seed=seed)\n",
    "                    for tick in range(run_length):\n",
    "                        model.step()\n",
    "                    model_data2 = model.datacollector.get_model_vars_dataframe()\n",
    "                    # Include all four variables in the key\n",
    "                    model_experiment[(i, rate, income_value, subsidy_value, flood_value)].append(model_data2)\n",
    "\n",
    "# Create a dictionary to hold the concatenated dataframes\n",
    "concatenated_dfs = {}\n",
    "\n",
    "# Concatenate the dataframes for each key\n",
    "for key, result_list in model_experiment.items():\n",
    "    # The key now includes all four variables\n",
    "    concatenated_dfs[key] = pd.concat(result_list)\n",
    "\n",
    "# Convert the dictionary of dataframes to a single dataframe\n",
    "# The key names will become column names\n",
    "model_dataframe_experiment = pd.concat(concatenated_dfs, names=['replication/seed', 'rate', 'income_value', 'subsidy_value', 'flood_value', 'Step'])\n",
    "\n",
    "# Reset the index\n",
    "model_dataframe_experiment.reset_index(inplace=True)\n",
    "\n",
    "# Save the results to csv files\n",
    "model_dataframe_experiment.to_csv(f\"../result_experiment/model_experiment.csv\", mode='a', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters for the sensitivity analysis\n",
    "subsidy_rate = [0.67]   # subsidy rate offered\n",
    "income_threshold = [3000, 6000, 9000, 12000] # agents which have lower income then the threshold eligible for subsidy\n",
    "saving_threshold = [0, 0.25, 0.75, 1] # agent calculate_saving(self) threshold, for saving or consuming\n",
    "flood_probabilities = [0.02, 0.07, 0.3] # different flood probabilities for harvey map\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "model_experiment = defaultdict(list)\n",
    "\n",
    "monitor = 0\n",
    "for rate in saving_threshold:\n",
    "    for income_value in income_threshold:\n",
    "        for subsidy_value in subsidy_rate:\n",
    "            for flood_value in flood_probabilities:\n",
    "                for i in range(replication_number):\n",
    "                    seed = i  # set the seed\n",
    "                    monitor += 1\n",
    "                    print('this is run of', monitor)\n",
    "                    model = AdaptationModel(number_of_households=number_of_households, subsidy_rate=subsidy_value, income_threshold=income_value, saving_threshold=rate, harvey_probability=flood_value, flood_map_choice=\"harvey\", network=\"no_network\", seed=seed)\n",
    "                    for tick in range(run_length):\n",
    "                        model.step()\n",
    "                    model_data2 = model.datacollector.get_model_vars_dataframe()\n",
    "                    # Include all four variables in the key\n",
    "                    model_experiment[(i, rate, income_value, subsidy_value, flood_value)].append(model_data2)\n",
    "\n",
    "# Create a dictionary to hold the concatenated dataframes\n",
    "concatenated_dfs = {}\n",
    "\n",
    "# Concatenate the dataframes for each key\n",
    "for key, result_list in model_experiment.items():\n",
    "    # The key now includes all four variables\n",
    "    concatenated_dfs[key] = pd.concat(result_list)\n",
    "\n",
    "# Convert the dictionary of dataframes to a single dataframe\n",
    "# The key names will become column names\n",
    "model_dataframe_experiment = pd.concat(concatenated_dfs, names=['replication/seed', 'rate', 'income_value', 'subsidy_value', 'flood_value', 'Step'])\n",
    "\n",
    "# Reset the index\n",
    "model_dataframe_experiment.reset_index(inplace=True)\n",
    "\n",
    "# Save the results to csv files\n",
    "model_dataframe_experiment.to_csv(f\"../result_experiment/model_experiment.csv\", mode='a', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters for the sensitivity analysis\n",
    "subsidy_rate = [0]   # subsidy rate offered\n",
    "income_threshold = [3000, 6000, 9000, 12000] # agents which have lower income then the threshold eligible for subsidy\n",
    "saving_threshold = [0, 0.25, 0.75, 1] # agent calculate_saving(self) threshold, for saving or consuming\n",
    "flood_probabilities = [0.02, 0.07, 0.3] # different flood probabilities for harvey map\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "model_experiment = defaultdict(list)\n",
    "\n",
    "monitor = 0\n",
    "for rate in saving_threshold:\n",
    "    for income_value in income_threshold:\n",
    "        for subsidy_value in subsidy_rate:\n",
    "            for flood_value in flood_probabilities:\n",
    "                for i in range(replication_number):\n",
    "                    seed = i  # set the seed\n",
    "                    monitor += 1\n",
    "                    print('this is run of', monitor)\n",
    "                    model = AdaptationModel(number_of_households=number_of_households, subsidy_rate=subsidy_value, income_threshold=income_value, saving_threshold=rate, harvey_probability=flood_value, flood_map_choice=\"harvey\", network=\"no_network\", seed=seed)\n",
    "                    for tick in range(run_length):\n",
    "                        model.step()\n",
    "                    model_data2 = model.datacollector.get_model_vars_dataframe()\n",
    "                    # Include all four variables in the key\n",
    "                    model_experiment[(i, rate, income_value, subsidy_value, flood_value)].append(model_data2)\n",
    "\n",
    "# Create a dictionary to hold the concatenated dataframes\n",
    "concatenated_dfs = {}\n",
    "\n",
    "# Concatenate the dataframes for each key\n",
    "for key, result_list in model_experiment.items():\n",
    "    # The key now includes all four variables\n",
    "    concatenated_dfs[key] = pd.concat(result_list)\n",
    "\n",
    "# Convert the dictionary of dataframes to a single dataframe\n",
    "# The key names will become column names\n",
    "model_dataframe_experiment = pd.concat(concatenated_dfs, names=['replication/seed', 'rate', 'income_value', 'subsidy_value', 'flood_value', 'Step'])\n",
    "\n",
    "# Reset the index\n",
    "model_dataframe_experiment.reset_index(inplace=True)\n",
    "\n",
    "# Save the results to csv files\n",
    "model_dataframe_experiment.to_csv(f\"../result_experiment/model_experiment.csv\", mode='a', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is run of 1\n",
      "this is run of 2\n",
      "this is run of 3\n",
      "this is run of 4\n",
      "this is run of 5\n",
      "this is run of 6\n",
      "this is run of 7\n",
      "this is run of 8\n",
      "this is run of 9\n",
      "this is run of 10\n",
      "this is run of 11\n",
      "this is run of 12\n",
      "this is run of 13\n",
      "this is run of 14\n",
      "this is run of 15\n",
      "this is run of 16\n",
      "this is run of 17\n",
      "this is run of 18\n",
      "this is run of 19\n",
      "this is run of 20\n",
      "this is run of 21\n",
      "this is run of 22\n",
      "this is run of 23\n",
      "this is run of 24\n",
      "this is run of 25\n",
      "this is run of 26\n",
      "this is run of 27\n",
      "this is run of 28\n",
      "this is run of 29\n",
      "this is run of 30\n",
      "this is run of 31\n",
      "this is run of 32\n",
      "this is run of 33\n",
      "this is run of 34\n",
      "this is run of 35\n",
      "this is run of 36\n",
      "this is run of 37\n",
      "this is run of 38\n",
      "this is run of 39\n",
      "this is run of 40\n",
      "this is run of 41\n",
      "this is run of 42\n",
      "this is run of 43\n",
      "this is run of 44\n",
      "this is run of 45\n",
      "this is run of 46\n",
      "this is run of 47\n",
      "this is run of 48\n",
      "this is run of 49\n",
      "this is run of 50\n",
      "this is run of 51\n",
      "this is run of 52\n",
      "this is run of 53\n",
      "this is run of 54\n",
      "this is run of 55\n",
      "this is run of 56\n",
      "this is run of 57\n",
      "this is run of 58\n",
      "this is run of 59\n",
      "this is run of 60\n",
      "this is run of 61\n",
      "this is run of 62\n",
      "this is run of 63\n",
      "this is run of 64\n",
      "this is run of 65\n",
      "this is run of 66\n",
      "this is run of 67\n",
      "this is run of 68\n",
      "this is run of 69\n",
      "this is run of 70\n",
      "this is run of 71\n",
      "this is run of 72\n",
      "this is run of 73\n",
      "this is run of 74\n",
      "this is run of 75\n",
      "this is run of 76\n",
      "this is run of 77\n",
      "this is run of 78\n",
      "this is run of 79\n",
      "this is run of 80\n",
      "this is run of 81\n",
      "this is run of 82\n",
      "this is run of 83\n",
      "this is run of 84\n",
      "this is run of 85\n",
      "this is run of 86\n",
      "this is run of 87\n",
      "this is run of 88\n",
      "this is run of 89\n",
      "this is run of 90\n",
      "this is run of 91\n",
      "this is run of 92\n",
      "this is run of 93\n",
      "this is run of 94\n",
      "this is run of 95\n",
      "this is run of 96\n",
      "this is run of 97\n",
      "this is run of 98\n",
      "this is run of 99\n",
      "this is run of 100\n",
      "this is run of 101\n",
      "this is run of 102\n",
      "this is run of 103\n",
      "this is run of 104\n",
      "this is run of 105\n",
      "this is run of 106\n",
      "this is run of 107\n",
      "this is run of 108\n",
      "this is run of 109\n",
      "this is run of 110\n",
      "this is run of 111\n",
      "this is run of 112\n",
      "this is run of 113\n",
      "this is run of 114\n",
      "this is run of 115\n",
      "this is run of 116\n",
      "this is run of 117\n",
      "this is run of 118\n",
      "this is run of 119\n",
      "this is run of 120\n",
      "this is run of 121\n",
      "this is run of 122\n",
      "this is run of 123\n",
      "this is run of 124\n",
      "this is run of 125\n",
      "this is run of 126\n",
      "this is run of 127\n",
      "this is run of 128\n",
      "this is run of 129\n",
      "this is run of 130\n",
      "this is run of 131\n",
      "this is run of 132\n",
      "this is run of 133\n",
      "this is run of 134\n",
      "this is run of 135\n",
      "this is run of 136\n",
      "this is run of 137\n",
      "this is run of 138\n",
      "this is run of 139\n",
      "this is run of 140\n",
      "this is run of 141\n",
      "this is run of 142\n",
      "this is run of 143\n",
      "this is run of 144\n",
      "this is run of 145\n",
      "this is run of 146\n",
      "this is run of 147\n",
      "this is run of 148\n",
      "this is run of 149\n",
      "this is run of 150\n",
      "this is run of 151\n",
      "this is run of 152\n",
      "this is run of 153\n",
      "this is run of 154\n",
      "this is run of 155\n",
      "this is run of 156\n",
      "this is run of 157\n",
      "this is run of 158\n",
      "this is run of 159\n",
      "this is run of 160\n",
      "this is run of 161\n",
      "this is run of 162\n",
      "this is run of 163\n",
      "this is run of 164\n",
      "this is run of 165\n",
      "this is run of 166\n",
      "this is run of 167\n",
      "this is run of 168\n",
      "this is run of 169\n",
      "this is run of 170\n",
      "this is run of 171\n",
      "this is run of 172\n",
      "this is run of 173\n",
      "this is run of 174\n",
      "this is run of 175\n",
      "this is run of 176\n",
      "this is run of 177\n",
      "this is run of 178\n",
      "this is run of 179\n",
      "this is run of 180\n",
      "this is run of 181\n",
      "this is run of 182\n",
      "this is run of 183\n",
      "this is run of 184\n",
      "this is run of 185\n",
      "this is run of 186\n",
      "this is run of 187\n",
      "this is run of 188\n",
      "this is run of 189\n",
      "this is run of 190\n",
      "this is run of 191\n",
      "this is run of 192\n",
      "this is run of 193\n",
      "this is run of 194\n",
      "this is run of 195\n",
      "this is run of 196\n",
      "this is run of 197\n",
      "this is run of 198\n",
      "this is run of 199\n",
      "this is run of 200\n",
      "this is run of 201\n",
      "this is run of 202\n",
      "this is run of 203\n",
      "this is run of 204\n",
      "this is run of 205\n",
      "this is run of 206\n",
      "this is run of 207\n",
      "this is run of 208\n",
      "this is run of 209\n",
      "this is run of 210\n",
      "this is run of 211\n",
      "this is run of 212\n",
      "this is run of 213\n",
      "this is run of 214\n",
      "this is run of 215\n",
      "this is run of 216\n",
      "this is run of 217\n",
      "this is run of 218\n",
      "this is run of 219\n",
      "this is run of 220\n",
      "this is run of 221\n",
      "this is run of 222\n",
      "this is run of 223\n",
      "this is run of 224\n",
      "this is run of 225\n",
      "this is run of 226\n",
      "this is run of 227\n",
      "this is run of 228\n",
      "this is run of 229\n",
      "this is run of 230\n",
      "this is run of 231\n",
      "this is run of 232\n",
      "this is run of 233\n",
      "this is run of 234\n",
      "this is run of 235\n",
      "this is run of 236\n",
      "this is run of 237\n",
      "this is run of 238\n",
      "this is run of 239\n",
      "this is run of 240\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the sensitivity analysis\n",
    "subsidy_rate = [1]   # subsidy rate offered\n",
    "income_threshold = [3000, 6000, 9000, 12000] # agents which have lower income then the threshold eligible for subsidy\n",
    "saving_threshold = [0, 0.25, 0.75, 1] # agent calculate_saving(self) threshold, for saving or consuming\n",
    "flood_probabilities = [0.02, 0.07, 0.3] # different flood probabilities for harvey map\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "model_experiment = defaultdict(list)\n",
    "\n",
    "monitor = 0\n",
    "for rate in saving_threshold:\n",
    "    for income_value in income_threshold:\n",
    "        for subsidy_value in subsidy_rate:\n",
    "            for flood_value in flood_probabilities:\n",
    "                for i in range(replication_number):\n",
    "                    seed = i  # set the seed\n",
    "                    monitor += 1\n",
    "                    print('this is run of', monitor)\n",
    "                    model = AdaptationModel(number_of_households=number_of_households, subsidy_rate=subsidy_value, income_threshold=income_value, saving_threshold=rate, harvey_probability=flood_value, flood_map_choice=\"harvey\", network=\"no_network\", seed=seed)\n",
    "                    for tick in range(run_length):\n",
    "                        model.step()\n",
    "                    model_data2 = model.datacollector.get_model_vars_dataframe()\n",
    "                    # Include all four variables in the key\n",
    "                    model_experiment[(i, rate, income_value, subsidy_value, flood_value)].append(model_data2)\n",
    "\n",
    "# Create a dictionary to hold the concatenated dataframes\n",
    "concatenated_dfs = {}\n",
    "\n",
    "# Concatenate the dataframes for each key\n",
    "for key, result_list in model_experiment.items():\n",
    "    # The key now includes all four variables\n",
    "    concatenated_dfs[key] = pd.concat(result_list)\n",
    "\n",
    "# Convert the dictionary of dataframes to a single dataframe\n",
    "# The key names will become column names\n",
    "model_dataframe_experiment = pd.concat(concatenated_dfs, names=['replication/seed', 'rate', 'income_value', 'subsidy_value', 'flood_value', 'Step'])\n",
    "\n",
    "# Reset the index\n",
    "model_dataframe_experiment.reset_index(inplace=True)\n",
    "\n",
    "# Save the results to csv files\n",
    "model_dataframe_experiment.to_csv(f\"../result_experiment/model_experiment.csv\", mode='a', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T21:21:12.694089Z",
     "start_time": "2024-01-26T21:16:35.084850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
